<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://mattcrmx.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mattcrmx.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-08T22:33:52+00:00</updated><id>https://mattcrmx.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Computer Plumbing 101: The File Descriptors leak</title><link href="https://mattcrmx.github.io/blog/2024/file-descriptors-leak/" rel="alternate" type="text/html" title="Computer Plumbing 101: The File Descriptors leak"/><published>2024-05-07T18:11:00+00:00</published><updated>2024-05-07T18:11:00+00:00</updated><id>https://mattcrmx.github.io/blog/2024/file-descriptors-leak</id><content type="html" xml:base="https://mattcrmx.github.io/blog/2024/file-descriptors-leak/"><![CDATA[<p>In my (quite short) experience as a software engineer, I’ve already encountered memory leaks more than once, whether it be my fault (for example when using FastAPI in synchronous mode with a RAM-heavy use case, perhaps this should actually be another blog post altogether) or some third party library’s fault. But I’ve actually grown fond of debugging those, as they’re sometimes the symptoms of bad software design that can be corrected at the same time. This blog post is about the first type of leak I encountered, the one that made me love debugging performance issues (especially lower level ones) : the file descriptors leak.</p> <h1 id="the-beginning">The Beginning</h1> <p>When working with pdf libraries, or libraries that involve managing resources (especially the ones leveraging bindings with lower level languages) one must be cautious that open resources are actually closed after having been used. Unfortunately, sometimes those libraries are not thread-safe, inducing unpredictable side effects if not managed correctly. Well, this was one of those times. In an early release of the excellent wrapper <a href="https://github.com/pypdfium2-team/pypdfium2"><code class="language-plaintext highlighter-rouge">pypdfium2</code></a>, the <a href="https://pdfium.googlesource.com/pdfium/"><code class="language-plaintext highlighter-rouge">Pdfium</code></a> library was not thread safe on two functions that I used in my code, and I had absolutely no idea. This code was running under several gunicorn workers in <code class="language-plaintext highlighter-rouge">gthread</code> with a lot of threads, and of course, this was the perfect setting for a good old race condition.</p> <p>At first, I did not understand: the memory kept on growing and I had thoroughly checked that the pdf object (wrapper and raw object) had been closed! But then I let the load test run for a longer period of time, and sure enough I hit <code class="language-plaintext highlighter-rouge">ulimit</code>! As I was starting to understand where it could come from, I thought I’d build myself a convenient tool to better identify what was actually happening.</p> <h1 id="reproducing-the-leak">Reproducing the leak</h1> <p>If you want to easily reproduce the leak at home (why would you though ?), here’s a nifty script to do so</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">base_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">path/to/data/folder</span><span class="sh">"</span>
    <span class="n">file_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">base_path</span><span class="p">):</span>
        <span class="n">file_dict</span><span class="p">[</span><span class="nb">file</span><span class="p">]</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="nb">file</span><span class="p">))</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">open</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>With this script, we can notice a steady increase on the number of file descriptors (the blue numbers on the image):</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fd_leak_proc-480.webp 480w,/assets/img/fd_leak_proc-800.webp 800w,/assets/img/fd_leak_proc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/fd_leak_proc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <p align="center"> This is what directly looking in the file descriptors folder of the process looks like</p> <h1 id="a-tailored-tool-fd-watcher">A tailored tool: fd-watcher</h1> <p>Since I thought that this tool should have a minimal time and memory overhead, I thought I’d write it in <code class="language-plaintext highlighter-rouge">C</code>. To be frank, this was an excuse to write some <code class="language-plaintext highlighter-rouge">C</code>, since I had always been fascinated by this language seeing my father write an absurd amount of code and always complaining about how easy it was to mess things up incredibly fast.</p> <p>The logic behind this tool is pretty simple: as we suspect that the descriptors are leaking, we need to find a way to see if all descriptors that are open by a process at some point end up being closed. To simplify, we can actually only look at the evolution of the number of descriptors simultaneously opened by a process, and see if this number is increasing (and even exploding if there’s a leak).</p> <p>To do so, assuming we have sufficient privileges, we can look at /proc/<pid> where a lot of information on the process is located, and in particular, a convenient folder called fd, where descriptors are opened and closed! We then only have to list the number of files in that folder at a fixed interval and see if that number goes up !</pid></p> <p>Ah, I see you wondering: “but wouldn’t it just be <code class="language-plaintext highlighter-rouge">ls /proc/&lt;pid&gt; | wc -l</code> in bash Matthias ? Why did you bother to write 500 lines of <code class="language-plaintext highlighter-rouge">C</code> for this ?” Well you’re right, but it was fun and actually taught me an awful lot about C and its most incredible friend: stack buffer overflow and <code class="language-plaintext highlighter-rouge">gdb</code>.</p> <p>This is what the tool looks like for a leaky process:</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fd_watcher-480.webp 480w,/assets/img/fd_watcher-800.webp 800w,/assets/img/fd_watcher-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/fd_watcher.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </p> <p>Using this tool is extremely simple, you can monitor a process by name or by pid and specify a duration and the interval for the monitoring and it will output the number of opened descriptors in the console.</p> <h1 id="future-developments">Future Developments</h1> <p>Being extremely naive and simple in its implementation, it’s not incredibly well coded while being functional, so a little overhaul of the codebase would clearly be nice. Mainly, I think that this tool is missing a graphical/visualization part, especially since it’s interesting to see at what rate the descriptors are leaking to identify the part of the code responsible for the leak. Maybe refactoring the codebase could also be nice, and making it a debian package/adding bindings to a small python package could ease the development and the integration in other monitoring stacks. I’ll try to implement that as soon as I can !</p> <h1 id="the-grand-finale">The Grand Finale</h1> <p>When I actually used <code class="language-plaintext highlighter-rouge">fd-watcher</code> on my use case and it revealed a gigantic leak of file descriptors, I couldn’t believe my eyes: I had coded something useful for once and it had shown the source of the problem! Then of course the much less flashy part of the work began where I had to put locks before the critical functions, but I was happy anyway.</p> <p>PS: Please build and share tools even if you think they’re useless, this seemingly little thing is actually the most useful thing I’ve built until now and I’m still using it whenever I have a leak to rule out descriptors.</p> <p>Check it out if you want !</p> <p align="center"> <img class="repo-img-dark w-100" width="50%" alt="Mattcrmx/fd-watcher" src="https://github-readme-stats.vercel.app/api/pin/?username=Mattcrmx&amp;repo=fd-watcher&amp;theme=dark&amp;show_owner=true"/> </p>]]></content><author><name></name></author><category term="plumbing"/><category term="memory"/><category term="leak"/><summary type="html"><![CDATA[A post about file descriptors leak in python]]></summary></entry></feed>